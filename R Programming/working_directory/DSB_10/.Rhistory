write_csv(m4, "sum_m4.csv")
## join in R
left_join(band_members, band_instruments, by = "name")
inner_join(band_members, band_instruments, by = "name")
full_join(band_members, band_instruments, by = "name")
m5 <- band_members %>%
select(member_name = name,
band_name = band) %>%
left_join(band_instruments,
by = c("member_name" = "name"))
## random sampling
# pull() pull column values only in a vector
mtcars %>%
sample_n(5) %>%
pull(model)
mtcars %>%
# sample_frac(0.20) %>%
summarise(avg_hp = mean(hp))
## summary
## 100% => Analytics
## 20% => Statistics
## count
mtcars <- mtcars %>%
mutate(am = if_else(am==0,
"Auto",
"Manual"))
ggplot(data = mtcars,
mapping = aes(x = mpg)) +
geom_histogram(bins=8)
library(tidyverse)
library(ggthemes)
ggplot(data = mtcars,
mapping = aes(x = mpg)) +
geom_histogram(bins=8)
ggplot(diamonds,
aes(x=price)) +
geom_histogram()
ggplot(diamonds,
aes(x=price)) +
geom_density()
## DRY: Donot Repeat Yourself
base <- ggplot(diamonds, aes(x=price))
p1 <- base + geom_histogram()
p2 <- base + geom_density()
## Boxplot
ggplot(diamonds, aes(x=price)) +
geom_boxplot()
## one variable + discrete (factor)
ggplot(diamonds, aes(cut)) +
geom_bar()
diamonds %>%
count(cut) %>%
ggplot(data = ., mapping=aes(x=cut, y=n)) +
geom_col()
set.seed(99)
ggplot(diamonds %>% sample_n(500),
# mapping
mapping = aes(x=carat, y=price,
color=cut)) +
# setting
geom_point(alpha=0.4, size=2) +
labs(
title="My first scatter plot",
subtitle="Cool ggplot2",
caption="Data: diamonds in Africa",
y = "Price in USD",
x = "Carat Diamonds"
) +
theme_minimal()
qplot(x = carat, data = diamonds, geom="density")
qplot(x = carat, data = diamonds, geom="histogram")
qplot(cut, data = diamonds, geom="bar")
## layers in ggplot2
base <- ggplot(diamonds %>%
sample_n(1000) %>%
filter(carat <= 2.8),
aes(x=carat, y=price))
p3 <- base +
theme_minimal() +
geom_point(alpha=0.7, color="#d2e80e") +
geom_smooth(method="loess", se=TRUE)
## bar plot
ggplot(diamonds, aes(cut, fill=clarity)) +
geom_bar(position="fill") +
theme_minimal()
## How to change color?
ggplot(diamonds, aes(cut, fill=cut)) +
geom_bar() +
theme_minimal() +
scale_fill_brewer(palette = "Accent")
## heat map color scale
ggplot(diamonds %>%
sample_frac(0.1), aes(carat, price, color=price)) +
geom_point(alpha=0.3) +
theme_minimal() +
scale_color_gradient(low = "gold", high = "purple")
ggplot(diamonds %>%
sample_frac(0.2), aes(carat,price)) +
geom_point(alpha=0.2, size=2, color="red") +
geom_smooth(method="lm", color="black") +
theme_minimal() +
facet_grid(~ cut)
m1 <- mtcars %>% filter(mpg > 30)
m2 <- mtcars %>% filter(mpg <= 20)
ggplot() +
theme_minimal() +
geom_point(data=m1, aes(wt, mpg), color="blue") +
geom_point(data=m2, aes(wt, mpg), color="red")
## bin2D
ggplot(diamonds, aes(carat, price)) +
geom_bin2d(bins=50)
library(patchwork)
p1 <- qplot(hwy, data=mpg, geom="density")
p2 <- qplot(cty, data=mpg, geom="histogram")
p3 <- qplot(cty, hwy, data=mpg, geom="point")
p4 <- qplot(cty, hwy, data=mpg, geom="smooth")
(p1 + p2 + p3) / p4
p1 / (p2 + p3 + p4)
View(my_profile)
knitr::opts_chunk$set(echo = TRUE)
## correlation
cor(mtcars$mpg, mtcars$hp)
## correlation
cor(mtcars$mpg, mtcars$hp)
## correlation
cor(mtcars$mpg, mtcars$hp)
## correlation
cor(mtcars$mpg, mtcars$hp)
head(mtcars)
cor(mtcars$mpg, mtcars$wt)
## correlation
cor(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$wt)
plot(mtcars$hp, mtcars$mpg)
plot(mtcars$hp, mtcars$mpg)
## correlation
cor(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$wt)
plot(mtcars$hp, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$hp, pch=16)
## correlation
cor(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$wt)
plot(mtcars$hp, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$hp, pch=16)
cor(mtcars[, c("mpg", "wt", "hp")])
## dplyr (tidyverse)
library(tidyverse)
mtcars %>%
select(mpg, wt, hp) %>%
cor()
c
corMAT <- mtcars %>%
select(mpg, wt, hp) %>%
cor()
corMAT
cor.test(mtcars$hp, mtcars$mpg)
## mpg = f(hp)
lm(mpg ~ hp, data = mtcars)
## mpg = f(hp)
lmFIT <- lm(mpg ~ hp, data = mtcars)
summary(lmFIT)
lmFIT$coefficients
lmFIT$coefficients[[1]] + lmFIT$coefficients[[2]]*200
new_cars <- data_frame(
hp = c(250, 220, 400, 450, 350)
)
new_cars <- data.frame(
hp = c(250, 220, 400, 450, 350)
)
new_cars
## predict
predict(lmFIT, newdata = new_cars)
## predict
new_cars$hp_predict <- predict(lmFIT, newdata = new_cars)
new_cars
new_cars <- data.frame(
hp = c(250, 220, 400, 450, 350)
)
new_cars
new_cars <- data.frame(
hp = c(250, 220, 400, 450, 350)
)
## predict
new_cars$mpg_predict <- predict(lmFIT, newdata = new_cars)
new_cars$hp_predict < NULL
new_cars
summary(new_cars$hp)
summary(mtcars$hp)
head(mtcars)
lmFIT2 <- lm.fit(mpg ~ hp + wt + am, data = mtcars)
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
head(mtcars)
## Multiple Linear Regression
## mpg = intercept + b0*hp + b1*wt + b2*am
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
lmFIT2 <- lm(mpg ~ hp + wt + am, mtcars)
lmFIT2 <- lm(mtcars$mpg ~ mtcars$hp + mtcars$wt + mtcars$am, mtcars)
lmFIT2 <- lm(mtcars$mpg ~ mtcars$hp + mtcars$wt + mtcars$am, mtcars)
## Build Full Model
lmFULL <- lm(mpg ~ ., data = mtcars)
mtcars$pred <- predict(lmFULL)
mtcars
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
#count unique values for each variable
lapply(df[c('hp', 'wt', 'am')], unique)
#count unique values for each variable
lapply(mtcars[c('hp', 'wt', 'am')], unique)
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
mtcars$am
mtcars
## correlation
cor(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$wt)
plot(mtcars$hp, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$hp, pch=16)
cor(mtcars[, c("mpg", "wt", "hp")])
## dplyr (tidyverse)
library(tidyverse)
corMAT <- mtcars %>%
select(mpg, wt, hp) %>%
cor()
cor.test(mtcars$hp, mtcars$mpg)
## mpg = f(hp)
lmFIT <- lm(mpg ~ hp, data = mtcars)
summary(lmFIT)
## predicction 1 veh
lmFIT$coefficients[[1]] + lmFIT$coefficients[[2]]*200
new_cars <- data.frame(
hp = c(250, 220, 400, 450, 350)
)
## predict
new_cars$mpg_predict <- predict(lmFIT, newdata = new_cars)
new_cars$hp_predict < NULL
summary(mtcars$hp)
head(mtcars)
## Multiple Linear Regression
## mpg = intercept + b0*hp + b1*wt + b2*am
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
coef <- coef(lmFIT2)
coef[[1]] + coef[[2]]*200 + coef[[3]]*3.5 + coef[[4]]*1
## Build Full Model
lmFULL <- lm(mpg ~ ., data = mtcars)
mtcars$pred <- predict(lmFULL)
head(mtcars)
## RMSE
error <- mtcars$mpg - mtcars$pred
## RMSE
error <- (mtcars$mpg - mtcars$pred)**2
rmse <- sqrt(mean(error))
knitr::opts_chunk$set(echo = TRUE)
## correlation
cor(mtcars$mpg, mtcars$hp)
cor(mtcars$mpg, mtcars$wt)
plot(mtcars$hp, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$mpg, pch=16)
plot(mtcars$wt, mtcars$hp, pch=16)
cor(mtcars[, c("mpg", "wt", "hp")])
## dplyr (tidyverse)
library(tidyverse)
corMAT <- mtcars %>%
select(mpg, wt, hp) %>%
cor()
cor.test(mtcars$hp, mtcars$mpg)
## mpg = f(hp)
lmFIT <- lm(mpg ~ hp, data = mtcars)
summary(lmFIT)
## predicction 1 veh
lmFIT$coefficients[[1]] + lmFIT$coefficients[[2]]*200
new_cars <- data.frame(
hp = c(250, 220, 400, 450, 350)
)
## predict
new_cars$mpg_predict <- predict(lmFIT, newdata = new_cars)
new_cars$hp_predict < NULL
summary(mtcars$hp)
head(mtcars)
## Multiple Linear Regression
## mpg = intercept + b0*hp + b1*wt + b2*am
lmFIT2 <- lm(mpg ~ hp + wt + am, data = mtcars)
coef <- coef(lmFIT2)
coef[[1]] + coef[[2]]*200 + coef[[3]]*3.5 + coef[[4]]*1
## Build Full Model
lmFULL <- lm(mpg ~ ., data = mtcars)
mtcars$pred <- predict(lmFULL)
head(mtcars)
## RMSE
error <- (mtcars$mpg - mtcars$pred)**2
rmse <- sqrt(mean(error))
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## Train Model
model1 <- lm(mpg ~ hp + wt, data = train_data)
p_train <- predict(model)
p_train <- predict(model1)
(rmse <- sqrt(mean(train_data$mpg - p_train)**2))
View(train_data)
(rmse <- sqrt( mean( (train_data$mpg - p_train)**2  ) ))
error <- train_data$mpg - p_train
## Train Model
model1 <- lm(mpg ~ hp + wt, data = train_data)
p_train <- predict(model1)
error_train <- train_data$mpg - p_train
(rmse_train <- sqrt( mean( error_train**2  ) ))
## Test Model
p_test <- predict(model, newdata=test_data)
## Test Model
p_test <- predict(model1, newdata=test_data)
error_test <- test_data$mpg - p_test
(rmse_train <- sqrt( mean( error_test**2  ) ))
model1 <- lm(mpg ~ hp + wt, data = train_data)
p_train <- predict(model1)
error_train <- train_data$mpg - p_train
(rmse_train <- sqrt( mean( error_train**2  ) ))
## Test Model
p_test <- predict(model1, newdata=test_data)
error_test <- test_data$mpg - p_test
(rmse_test <- sqrt( mean( error_test**2  ) ))
## Print results
cat("RMSE TRAIN:", rmse_train,
"\nRMSE_TEST:", rmse_test)
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## Train Model
model1 <- lm(mpg ~ hp + wt + am + disp, data = train_data)
p_train <- predict(model1)
error_train <- train_data$mpg - p_train
(rmse_train <- sqrt( mean( error_train**2  ) ))
## Test Model
p_test <- predict(model1, newdata=test_data)
error_test <- test_data$mpg - p_test
(rmse_test <- sqrt( mean( error_test**2  ) ))
## Print results
cat("RMSE TRAIN:", rmse_train,
"\nRMSE_TEST:", rmse_test) ## Overfitting Problem
## Train Model
model1 <- lm(mpg ~ hp + wt + am + disp, data = train_data)
p_train <- predict(model1)
error_train <- train_data$mpg - p_train
(rmse_train <- sqrt( mean( error_train**2  ) ))
## Test Model
p_test <- predict(model1, newdata=test_data)
error_test <- test_data$mpg - p_test
(rmse_test <- sqrt( mean( error_test**2  ) ))
View(model1)
## Logistic Regression
library(dplyr)
mtcars %>% head()
str(mtcars)
## Convert am to factorized string
mtcars$am %>% factor(mtcars$am,
levels = c(0, 1),
labels = c("Auto", "Manual"))
## Convert am to factorized string
mtcars$am <- factor(mtcars$am,
levels = c(0, 1),
labels = c("Auto", "Manual"))
class(mtcars$am)
table(mtcars$am)
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## train model
logitModel <- glm(am ~ mpg, data = train_data)
View(train_data)
## train model
logitModel <- glm(am ~ mpg, data = train_data)
## train model
logitModel <- glm(am ~ mpg, data = train_data, family = "binomial")
p_TrainLogit <- predict(logitModel)
## train model
logitModel <- glm(am ~ mpg, data = train_data, family = "binomial")
p_TrainLogit <- predict(logitModel, type = "response") ## probability
p_TrainLogit
train_data$pred <- if_else(p_TrainLogit >= 0.5, "Manual", "Auto")
train_data$pred
train_data$am == train_data$pred
mean(train_data$am == train_data$pred)
## Logistic Regression
library(dplyr)
mtcars %>% head()
str(mtcars)
## Convert am to factorized string
mtcars$am <- factor(mtcars$am,
levels = c(0, 1),
labels = c("Auto", "Manual"))
class(mtcars$am)
table(mtcars$am)
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## train model
logitModel <- glm(am ~ mpg, data = train_data, family = "binomial")
View(mtcars)
# Logistic Regression
library(dplyr)
mtcars %>% head()
str(mtcars)
## Convert am to factorized string
mtcars$am <- factor(mtcars$am,
levels = c(0, 1),
labels = c("Auto", "Manual"))
class(mtcars$am)
table(mtcars$am)
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## Logistic Regression
library(dplyr)
mtcars %>% head()
str(mtcars)
## Convert am to factorized string
mtcars$am <- factor(mtcars$am,
levels = c(0, 1),
labels = c("Auto", "Manual"))
class(mtcars$am)
table(mtcars$am)
## Split Data
set.seed(42)
n <- nrow(mtcars)
id <- sample(1:n, size = 0.8*n)
train_data <- mtcars[id, ]
test_data <- mtcars[-id, ]
## train model
logitModel <- glm(am ~ mpg, data = train_data, family = "binomial")
p_TrainLogit <- predict(logitModel, type = "response") ## probability
train_data$pred <- if_else(p_TrainLogit >= 0.5, "Manual", "Auto")
mean(train_data$am == train_data$pred)
## test model
p_TestLogit <- predict(logitModel, newdata = test_data,
type = "response") ## probability
test_data$pred <- if_else(p_TestLogit >= 0.5,
"Manual", "Auto")
mean(test_data$am == test_data$pred)
## Logistic Regression
happiness <- c(10, 8, 9, 7, 8, 5, 9, 6, 8, 7, 1, 1, 3, 1, 4, 5, 6, 3, 2, 0, )
divorce <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
df <- data.frame(happiness, divorce)
## Logistic Regression
happiness <- c(10, 8, 9, 7, 8, 5, 9, 6, 8, 7, 1, 1, 3, 1, 4, 5, 6, 3, 2, 0)
divorce <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
df <- data.frame(happiness, divorce)
rep(0, 10)
rep(0, 20)
## Fit Logistic Regression
glm(divorce ~ happiness, data = df, family = "binomial")
## Fit Logistic Regression
model <- glm(divorce ~ happiness, data = df, family = "binomial")
summary(model)
head(df)
## Predict and Evaluate model
df$prob_divorce <- predict(model, type = "response")
df
df$per_divorce <- df$prob_divorce * 100
df
df$pred_divorce <- ifelse(df$prob_divorce >= 0.5, 1, 0)
df
## confusion matrix
table(df$pred_divorce, df$divorce)
## confusion matrix
table(df$pred_divorce, df$divorce, dnn = c("Predicted", "Actual"))
## confusion matrix
confusion_matrix <- table(df$pred_divorce, df$divorce, dnn = c("Predicted", "Actual"))
## Model evaluation
confusion_matrix[1,1]
## Model evaluation
confusion_matrix[1,2]
## Model evaluation
confusion_matrix[2,2]
## Model evaluation
confusion_matrix[1,1]
confusion_matrix[1,2]
confusion_matrix[2,1]
confusion_matrix[2,2]
cat("Accuracy:", (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix))
acc <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix)
acc
confusion_matrix[1,1]
confusion_matrix[1,2]
confusion_matrix[2,1]
confusion_matrix[2,2]
cat("Accuracy:", (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix))
cat("Precision:",  confusion_matrix[2,2] / confusion_matrix[2,1] + confusion_matrix[2,2])
acc <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix)
cat("Accuracy:", (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix))
cat("Precision:",  confusion_matrix[2,2] / (confusion_matrix[2,1] + confusion_matrix[2,2]))
cat("Recall:",  confusion_matrix[2,2] / (confusion_matrix[1,2] + confusion_matrix[2,2]))
acc <- (confusion_matrix[1,1] + confusion_matrix[2,2]) / sum(confusion_matrix)
preci <- confusion_matrix[2,2] / (confusion_matrix[2,1] + confusion_matrix[2,2])
recall <- confusion_matrix[2,2] / (confusion_matrix[1,2] + confusion_matrix[2,2])
cat("F1:", 2*(preci*recall)/(preci + recall))
