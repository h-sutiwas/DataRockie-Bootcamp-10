}
count_ball(balls, c("red", "blue"))
# add_two_nums() function
add_two_nums <- function(val1, val2) {
val1 + val2
}
# cube() function
cube <- function(base, power=3) {
return(base ** power)
}
# count_ball() function
balls <- c("red", "red", "blue", "green",
"green", "green", "green", "red")
count_ball <- function(balls, color) {
sum(balls == color)
}
count_ball(balls, c("red"))
# add_two_nums() function
add_two_nums <- function(val1, val2) {
val1 + val2
}
# cube() function
cube <- function(base, power=3) {
return(base ** power)
}
# count_ball() function
balls <- c("red", "red", "blue", "green",
"green", "green", "green", "red")
count_ball <- function(balls, color) {
sum(balls == color)
}
count_ball(balls, "red")
count_ball(balls, "blue")
count_ball(balls, "green")
# refactor our code for more readability
cal_mean_by_col <- function(df) {
col_names <- names(df)
# we use [[]] to extract column as vector
for (i in 1:ncol(df)) {
avg_col <- mean(df[[i]])
print(paste(col_names[i], ":", avg_col))
}
}
# test our code with mtcars
cal_mean_by_col(mtcars)
# refactor our code for more readability
cal_mean_by_col <- function(df) {
col_names <- names(df)
# we use [[]] to extract column as vector
for (i in 1:ncol(df)) {
avg_col <- mean(df[[i]])
print(paste(col_names[i], ":", avg_col))
}
}
# test our code with mtcars
cal_mean_by_col(mtcars)
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean())
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean)
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean)
avg_by_row_mtcars[1:10]
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 2, mean)
avg_by_row_mtcars[1:10]
# apply function
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean)
avg_by_row_mtcars[1:10]
# apply function: MARGIN = (1: By rows, 2: By columns)
avg_by_row_mtcars <- apply(mtcars, MARGIN = 1, mean)
avg_by_row_mtcars[1:10]
knitr::opts_chunk$set(echo = TRUE)
# install packages
install.packages(c("readr",
"readxl",
"googlesheets4",
"jsonlite",
"dplyr",
"sqldf",
"RSQLite"))
# load library
library(readr)
library(readxl)
library(googlesheets4)
library(jsonlite)
library(dplyr)
library(sqldf)
library(RSQLite)
# read text file
student1 <- read_table("C:\Online Learning\DataRockie Bootcamp\DataRockie Bootcamp 10th\R Programming\data\student.txt")
# read text file
student1 <- read_table("cd \Online Learning\DataRockie Bootcamp\DataRockie Bootcamp 10th\R Programming\data\student.txt")
# read text file
student1 <- read_table("C/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/student.txt")
# read text file
student1 <- read_table("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/student.txt")
student1
# read csv file
student2 <- read_csv("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/student.csv")
student2
# read excel file
result <- list()
for (i in 1:3) {
result[[i]] <- read_excel("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/students.xlsx", sheet=i)
}
result[[1]]
result[[2]]
result[[3]]
# read JSON file
library(jsonlite)
bp <- data.frame(fromJSON("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/blackpink.json"))
View(bp)
library(googlesheets4)
url <- "https://googlesheets4.tidyverse.org/articles/auth.html"
gs4_deauth()
df <- read_sheet(url, sheet = "student")
library(googlesheets4)
url <- "https://googlesheets4.tidyverse.org/articles/auth.html"
gs4_deauth()
read_sheet(url, sheet = "student")
library(googlesheets4)
url <- "https://docs.google.com/spreadsheets/d/1y5JuaUhNrdRzf_hJrdZjZHARK6kHxvoVR5U4IJuGEyQ/edit?gid=1807022667#gid=1807022667"
gs4_deauth()
read_sheet(url, sheet = "Example Data")
library(googlesheets4)
url <- "https://docs.google.com/spreadsheets/d/1y5JuaUhNrdRzf_hJrdZjZHARK6kHxvoVR5U4IJuGEyQ/edit?usp=sharing"
gs4_deauth()
read_sheet(url, sheet = "Example Data")
# read JSON file
library(jsonlite)
bp <- data.frame(fromJSON("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/blackpink.json"))
View(bp)
# Bind Rows (UNION ALL in  SQL)
library(dplyr)
library(readxl)
## read excel file
econ <- read_excel("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/students.xlsx", sheet = 1)
business <- read_excel("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/students.xlsx", sheet = 2)
data <- read_excel("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/students.xlsx", sheet = 3)
## binding rows
list_df <- list(econ, business, data)
full_df <- bind_rows(list_df)
# Bind Cols (!= JOIN)
df1 <- data.frame(
id = 1:5,
name = c("John", "Marry", "Anna", "David", "Lisa")
)
df2 <- data.frame(
id = 1:5,
city = c(rep("BANGKOK", 3), rep("LONDON", 2)),
country = c(rep("TH", 3), rep("UK", 2))
)
# load library sqldf
library(sqldf)
library(readr)
school <- read_csv("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/school/school.csv")
sqldf("select * from school;")
sqldf("select avg(student), sum(student) from school;")
sqldf("select school_id, school_name, country from school;")
sql_query <- "select * from school where country = 'USA';"
usa_school <- sqldf(sql_query)
# load library
library(RSQLite)
# connect to SQLite database (.db file)
# 1. open connection
conn <- dbConnect(SQLite(), "C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/chinook/chinook.db")
# 2. get data
dbListTables(conn)
dbListFields(conn, "customers")
df <- dbGetQuery(conn, "select * from customers where country = 'USA'")
df2 <- dbGetQuery(conn, "select * from customers where country = 'United Kingdom'")
# 3. close connection
dbDisconnect(conn)
saveRDS(business, file = "business.rds")
readRDS("business.rds")
business <- readRDS("business.rds")
save.image()
knitr::opts_chunk$set(echo = TRUE)
## install packages
install.packages("dplyr")
## load packages
library(dplyr)
## read csv file into RStudio
imdb <- read.csv("C:/DataRockie/R Programming/directory/imdb/imdb.csv", stringsAsFactors = FALSE)
## install packages
install.packages("dplyr")
## load packages
library(dplyr)
## read csv file into RStudio
imdb <- read.csv("C:\Online Learning\DataRockie Bootcamp\DataRockie Bootcamp 10th\R Programming\data\imdb\imdb.csv", stringsAsFactors = FALSE)
install.packages("dplyr")
## read csv file into RStudio
imdb <- read.csv("C:\Online Learning\DataRockie Bootcamp\DataRockie Bootcamp 10th\R Programming\data\imdb\imdb.csv", stringsAsFactors = FALSE)
## read csv file into RStudio
imdb <- read.csv("C: /Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/imdb/imdb.csv", stringsAsFactors = FALSE)
## read csv file into RStudio
imdb <- read.csv("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/imdb/imdb.csv", stringsAsFactors = FALSE)
View(imdb)
## review data structure
glimpse(imdb)
## print head and tail of imdb data
head(imdb, 10)
## read csv file into RStudio
imdb <- read.csv("C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/imdb/imdb.csv", stringsAsFactors = FALSE)
View(imdb)
## review data structure
glimpse(imdb)
## print head and tail of imdb data
head(imdb, 10)
tail(imdb, 10)
# select columns
select(imdb, MOVIE_NAME, RATING)
select(imdb, 1, 5)
select(imdb, movie_name = MOVIE_NAME, release_year = YEAR)
## pipe operator %>% call function without select function
## easy to create data pipelines [df %>% select() %>% filter() %>% mutate() %>% arrange()]
head(imdb)
imdb %>%
select(movie_name = MOVIE_NAME, release_year = YEAR) %>%
head()
imdb %>%
select(MOVIE_NAME, RATING)
## filter data
filter(imdb, SCORE >= 9.0)
imdb %>% filter(SCORE >= 9.0)
names(imdb) <- tolower(names(imdb))
imdb %>%
select(movie_name, year, score) %>%
filter(score >= 9.0 & year > 2000)
imdb %>%
select(movie_name, year, score) %>%
filter(score == 8.8 | score == 8.3 | score == 9.0)
imdb %>%
select(movie_name, year, score) %>%
filter(score %in% c(8.3, 8.8, 9.0))
## filter string columns
# grepl("Drama", imdb$genre) help find "Drama" within genre string columns
# while ignoring their position or if said movie have other genre type inside their string.
imdb %>%
select(movie_name, genre, rating) %>%
filter(rating == "PG-13")
imdb %>%
select(movie_name, genre, rating) %>%
filter(grepl("Drama", imdb$genre))
imdb %>%
select(movie_name) %>%
filter(grepl("King", imdb$movie_name))
# case sensitive with all lowercase "king"
## arrange data
head(imdb)
imdb %>%
arrange(length) %>%
head(10)
imdb %>%
arrange(desc(length)) %>% ## descending order
head(10)
imdb %>%
arrange(rating, desc(length))
## create new columns
imdb %>%
select(movie_name, score, length) %>%
mutate(score_group = if_else(score >= 9, "High Rating", "Low Rating"),
length_group = if_else(length >= 120, "Long Film", "Short Film"))
imdb %>%
select(movie_name, score) %>%
mutate(score = score + 0.1) %>%
head(10)
## summarize and group by
# summarise(name_stat = stat_func())
# summarise stat groups by rating types
imdb %>%
filter(rating != "") %>%
group_by(rating) %>%
summarise(mean_length = mean(length),
sd_length = sd(length),
min_length = min(length),
max_length = max(length),
n = n()
)
## join data
favorite_films <- data.frame(id = c(5, 10, 25, 30, 98))
favorite_films %>%
inner_join(imdb, by = c("id" = "no"))
# --- Join table favorite_films with imdb by column "id"   ##
##              inside favorite_films with column "no" ---  #
## write csv file (export result)
imdb_prep <- imdb %>%
select(movie_name, released_year = year, rating, length, score) %>%
filter(rating == "R" & released_year > 2000)
## export csv file
write.csv(imdb_prep, "imdb_prep.csv", row.names = FALSE)
save.image()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RSQLite)
library(glue)
library(RPostgreSQL)
library(lubridate)
library(sqldf)
# library glue -> to glue messages
my_name <- "toy"
my_age <- 35
glue("Hi my name is {my_name}. Today I'm {my_age} years old.")
# library tidyverse
# data transformation => dplyr
# 1. select
# 2. filter
# 3. arrange
# 4. mutate => create new column
# 5. summarise
mtcars <- rownames_to_column(mtcars, "model")
view(mtcars)
# pipeline
select(mtcars, mpg, hp, wt)
m2 <- mtcars %>%
select(mpg, hp, wt) %>%
filter(hp > 200) %>%
arrange(mpg) %>%
head()
head(arrange(filter(select(mtcars, mpg, hp, wt), hp > 200), mpg))
# filter am == 0
# grep can return character value and normally return the index of said value
# but grepl() will always return logic value
mtcars %>%
select(model, mpg, hp, wt, am) %>%
filter(am == 0 & grepl("^M", model)) %>%
arrange(am, desc(hp))
m3 <- mtcars %>%
select(model, mpg, hp, wt, am) %>%
filter(between(hp, 100, 200)) %>%
arrange(am, desc(hp))
# write csv file
write_csv(m3, "result.csv")
# mutate to create new columns
mtcars %>%
filter(mpg >= 20) %>%
select(model, mpg, hp, wt, am) %>%
mutate(new_col = "Data Rockie",
hp_halve = hp/2,
## =IF(am==0,"auto", "manual")
am_label = if_else(am==0,"auto", "manual"))
# summarise, summarize
# aggregate function in SQL
m4 <- mtcars %>%
mutate(am = if_else(am==0, "auto", "manual")) %>%
group_by(am) %>%
summarise(avg_mpg = mean(mpg),
med_mpg = median(mpg),
sum_mpg = sum(mpg),
max_mpg = max(mpg),
max_hp = max(hp),
min_hp = min(hp),
# spread
n = n(),
# var_hp = var(hp),
# sd_hp = sd(hp)
)
write_csv(m4, "sum_m4.csv")
## join in R
left_join(band_members, band_instruments, by = "name")
inner_join(band_members, band_instruments, by = "name")
full_join(band_members, band_instruments, by = "name")
m5 <- band_members %>%
select(member_name = name,
band_name = band) %>%
left_join(band_instruments,
by = c("member_name" = "name"))
## random sampling
# pull() pull column values only in a vector
mtcars %>%
sample_n(5) %>%
pull(model)
mtcars %>%
# sample_frac(0.20) %>%
summarise(avg_hp = mean(hp))
## summary
## 100% => Analytics
## 20% => Statistics
## count
mtcars <- mtcars %>%
mutate(am = if_else(am==0,
"Auto",
"Manual"))
sqldf("
SELECT
mpg,
hp
FROM mtcars
WHERE hp >= 200")
library("sqldf")
sqldf("
SELECT
mpg,
hp
FROM mtcars
WHERE hp >= 200")
## create connection to sqlite.db file
con <- dbConnect(SQLite(), "C:\Online Learning\DataRockie Bootcamp\DataRockie Bootcamp 10th\R Programming\data\chinook")
## create connection to sqlite.db file
con <- dbConnect(SQLite(), "C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/chinook/chinook.db")
## list tables
dbListTables(con)
## list fields/ columns
dbListFields(con, "customers")
## get data from database tables
usa <- dbGetQuery(con, "select firstname, email from customers
where country = 'USA' ")
## create dataframe
products <- tribble(
~id, ~product_name,
1, "chocolate",
2, "pineapple",
3, "samsung galaxy s23"
)
## write table to database
dbWriteTable(con, "products", products)
## remove table
dbRemoveTable(con, "products")
## close connection
dbDisconnect(con)
## connect to PostgreSQL server
## create connection to elephantsql.com after building a database
# these parameters below are used as connector
con <- dbConnect(
PostgreSQL(),
host = "arjuna.db.elephantsql.com",
dbname = "hcmsanst",
user = "hcmsanst",
password = "n9MIT1wNqy6HG5HsO-ypvf8SLD4VfMCh",
port = 5432
)
## db List Tables
dbListTables(con)
dbWriteTable(con, "products", products)
## get data
df <- dbGetQuery(con, "select id, product_name from products")
## create connection to sqlite.db file
con <- dbConnect(SQLite(), "C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/chinook/chinook.db")
## list tables
dbListTables(con)
## list fields/ columns
dbListFields(con, "customers")
## get data from database tables
usa <- dbGetQuery(con, "
select
firstname,
lastname,
country,
email
from customers
where country = 'USA' ")
## create dataframe
products <- tribble(
~id, ~product_name,
1, "chocolate",
2, "pineapple",
3, "samsung galaxy s23"
)
## write table to database
dbWriteTable(con, "products", products)
## remove table
dbRemoveTable(con, "products")
## close connection
dbDisconnect(con)
## create connection to sqlite.db file
con <- dbConnect(SQLite(), "C:/Online Learning/DataRockie Bootcamp/DataRockie Bootcamp 10th/R Programming/data/chinook/chinook.db")
## list tables
dbListTables(con)
## list fields/ columns
dbListFields(con, "customers")
## get data from database tables
usa <- dbGetQuery(con, "
select
firstname,
lastname,
country,
email
from customers
where country = 'USA' ")
## create dataframe
products <- tribble(
~id, ~product_name,
1, "chocolate",
2, "pineapple",
3, "samsung galaxy s23"
)
## write table to database
dbWriteTable(con, "products", products)
## remove table
dbRemoveTable(con, "products")
## close connection
dbDisconnect(con)
usa
## connect to PostgreSQL server
## create connection to elephantsql.com after building a database
# these parameters below are used as connector
con <- dbConnect(
PostgreSQL(),
host = "arjuna.db.elephantsql.com",
dbname = "hcmsanst",
user = "hcmsanst",
password = "n9MIT1wNqy6HG5HsO-ypvf8SLD4VfMCh",
port = 5432
)
## db List Tables
dbListTables(con)
dbWriteTable(con, "products", products)
## get data
df <- dbGetQuery(con, "select id, product_name from products")
